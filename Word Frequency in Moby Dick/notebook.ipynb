{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":17,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')","executionCancelledAt":null,"lastExecutedAt":1737581258784,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":39}]},{"source":"# url to scrape\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n\n# request the Moby Dick HTML file using requests and encoding it to utf-8\nr = requests.get(url)\nr.encoding = \"utf-8\"","metadata":{"executionCancelledAt":null,"executionTime":61,"lastExecutedAt":1737581258845,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# url to scrape\nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n\n# request the Moby Dick HTML file using requests and encoding it to utf-8\nr = requests.get(url)\nr.encoding = \"utf-8\""},"cell_type":"code","id":"e71d8bae-7680-4bed-a0d7-028e7d42e4b6","outputs":[],"execution_count":40},{"source":"# Parse the HTML content with BeautifulSoup\nhtml = r.text\nhtml_soup = BeautifulSoup(html, 'html.parser')\n\n# Print the text content of the page\nprint(html_soup.get_text()[:500])  # Display the first 500 characters of the text","metadata":{"executionCancelledAt":null,"executionTime":444,"lastExecutedAt":1737581259289,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Parse the HTML content with BeautifulSoup\nhtml = r.text\nhtml_soup = BeautifulSoup(html, 'html.parser')\n\n# Print the text content of the page\nprint(html_soup.get_text()[:500])  # Display the first 500 characters of the text","outputsMetadata":{"0":{"height":542,"type":"stream"}}},"cell_type":"code","id":"19d91565-1933-45ea-8cce-b3baa867436e","outputs":[{"output_type":"stream","name":"stdout","text":"\n\n\n\n\n      Moby Dick; Or the Whale, by Herman Melville\n    \n\n\n\n\n\nThe Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n\nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.org\n\n\nTitle: Moby Dick; or The Whale\n\nAuthor: Herman Melville\n\nRelease Date: December 25, 2008 [EB\n"}],"execution_count":41},{"source":"# Extract the text from the HTML\nmoby_text = html_soup.get_text()\n\n# Initialize the regex tokenizer to keep only alphanumeric words\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\n# Print the first 20 tokens\nprint(tokens[:20])","metadata":{"executionCancelledAt":null,"executionTime":67,"lastExecutedAt":1737581259356,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract the text from the HTML\nmoby_text = html_soup.get_text()\n\n# Initialize the regex tokenizer to keep only alphanumeric words\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(moby_text)\n\n# Print the first 20 tokens\nprint(tokens[:20])","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"7fc5d06d-1b7d-4d37-9d18-72e0db3e7bf0","outputs":[{"output_type":"stream","name":"stdout","text":"['Moby', 'Dick', 'Or', 'the', 'Whale', 'by', 'Herman', 'Melville', 'The', 'Project', 'Gutenberg', 'EBook', 'of', 'Moby', 'Dick', 'or', 'The', 'Whale', 'by', 'Herman']\n"}],"execution_count":42},{"source":"# Convert tokens to lowercase\nwords = [token.lower() for token in tokens]\n\n# Load English stopwords\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Remove stop words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first 20 words after stop word removal\nprint(words_no_stop[:20])","metadata":{"executionCancelledAt":null,"executionTime":408,"lastExecutedAt":1737581259764,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert tokens to lowercase\nwords = [token.lower() for token in tokens]\n\n# Load English stopwords\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Remove stop words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first 20 words after stop word removal\nprint(words_no_stop[:20])","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"b796a0b9-1d9a-45e0-b9f4-75a62a4979cc","outputs":[{"output_type":"stream","name":"stdout","text":"['moby', 'dick', 'whale', 'herman', 'melville', 'project', 'gutenberg', 'ebook', 'moby', 'dick', 'whale', 'herman', 'melville', 'ebook', 'use', 'anyone', 'anywhere', 'cost', 'almost', 'restrictions']\n"}],"execution_count":43},{"source":"# Initialize the Counter object to count word frequencies\ncount = Counter(words_no_stop)\n\n# Get the 10 most common words\ntop_ten = count.most_common(10)\n\n# Print the top 10 most common words\nprint(top_ten)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1737581259816,"lastExecutedByKernel":"00fbb1ee-2e7e-41d8-a898-57bce6ecfa15","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize the Counter object to count word frequencies\ncount = Counter(words_no_stop)\n\n# Get the 10 most common words\ntop_ten = count.most_common(10)\n\n# Print the top 10 most common words\nprint(top_ten)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"e7766ec5-322d-4334-a75a-408f56a00247","outputs":[{"output_type":"stream","name":"stdout","text":"[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}],"execution_count":44}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}